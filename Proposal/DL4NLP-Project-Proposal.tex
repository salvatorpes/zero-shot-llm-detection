\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2025

% The authors should use one of these tracks.
% Before accepting by the NeurIPS conference, select one of the options below.
% 0. "default" for submission
 \usepackage[main, final]{neurips_2025}
% the "default" option is equal to the "main" option, which is used for the Main Track with double-blind reviewing.
% 1. "main" option is used for the Main Track
%  \usepackage[main]{neurips_2025}
% 2. "position" option is used for the Position Paper Track
%  \usepackage[position]{neurips_2025}
% 3. "dandb" option is used for the Datasets & Benchmarks Track
 % \usepackage[dandb]{neurips_2025}
% 4. "creativeai" option is used for the Creative AI Track
%  \usepackage[creativeai]{neurips_2025}
% 5. "sglblindworkshop" option is used for the Workshop with single-blind reviewing
 % \usepackage[sglblindworkshop]{neurips_2025}
% 6. "dblblindworkshop" option is used for the Workshop with double-blind reviewing
%  \usepackage[dblblindworkshop]{neurips_2025}

% After being accepted, the authors should add "final" behind the track to compile a camera-ready version.
% 1. Main Track
 % \usepackage[main, final]{neurips_2025}
% 2. Position Paper Track
%  \usepackage[position, final]{neurips_2025}
% 3. Datasets & Benchmarks Track
 % \usepackage[dandb, final]{neurips_2025}
% 4. Creative AI Track
%  \usepackage[creativeai, final]{neurips_2025}
% 5. Workshop with single-blind reviewing
%  \usepackage[sglblindworkshop, final]{neurips_2025}
% 6. Workshop with double-blind reviewing
%  \usepackage[dblblindworkshop, final]{neurips_2025}
% Note. For the workshop paper template, both \title{} and \workshoptitle{} are required, with the former indicating the paper title shown in the title and the latter indicating the workshop title displayed in the footnote.
% For workshops (5., 6.), the authors should add the name of the workshop, "\workshoptitle" command is used to set the workshop title.
% \workshoptitle{WORKSHOP TITLE}

% "preprint" option is used for arXiv or other preprint submissions
 % \usepackage[preprint]{neurips_2025}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2025}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

% Note. For the workshop paper template, both \title{} and \workshoptitle{} are required, with the former indicating the paper title shown in the title and the latter indicating the workshop title displayed in the footnote. 
\title{DL4NLP - Project Proposal Group 15 \\ Zero-Shot Human vs AI Generated Text Detection using LLMs}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
%   David S.~Hippocampus\thanks{Use footnote for providing further information
%     about author (webpage, alternative address)---\emph{not} for acknowledging
%     funding agencies.} \\
%   Department of Computer Science\\
%   Cranberry-Lemon University\\
%   Pittsburgh, PA 15213 \\
%   \texttt{hippo@cs.cranberry-lemon.edu} \\
    Jose Carrilo \\
    University of Amsterdam\\
    \texttt{jose.garcia.carrillo@student.uva.nl} \\
    \And
    Krystof Bobek \\
    University of Amsterdam\\
    \texttt{krystof.bobek@student.uva.nl} \\
    \And
    Mara Dragomir \\
    University of Amsterdam\\
    \texttt{mara.dragomir@student.uva.nl} \\
    \And
    Mahdi Rahimi \\
    University of Amsterdam\\
    \texttt{mahdi.rahimi@student.uva.nl} \\
    \And
    Salvador Torpes \\
    University of Amsterdam\\
    \texttt{salvador.baptista.torpes@student.uva.nl} \\
}


\begin{document}

\maketitle


% \begin{abstract}
% \end{abstract}

\subsection*{Project Goal}

In recent years AI development has accelerated rapidly, leading to the creation of increasingly sophisticated agents that incorporate powerful language models. This progress has raised concerns about the potential misuse of these resources, in contexts such as misinformation, content generation and education. From this, the need for AI generated text detection arises as a tool to help identify and mitigate the impact of such misuse. Among multiple attempts to address this issue, Mitchell et al.\cite{detectgpt} proposed a zero-shot approach called \textit{DetectGPT} that leverages probability curvature of perturbed human and AI-generated text. This approach is based on the observation that the loss curvature is consistently negative for AI-generated text and not clearly negative or positive for human-written text. Mitchell et al.'s \cite{detectgpt} algorithm is based solely on the model's internal metrics, and it outperformed other methods including some that required fine-tuning and training, showing this approach's effectiveness. Further research lead Bao et al.\cite{fastdetectgpt} to propose \textit{Fast-DetectGPT}, an improvement to the original algorithm that uses sampling from a LLM in order to produce the perturbed versions of both AI and human generated text.

Our goal is to implement \textit{Fast-DetectGPT} and evaluate its performance in detecting AI-generated text using new models and a new dataset. Furthermore, some extensions may be explored in order to understand what are the limitations of this zero-shot approach. 

\paragraph{Research Questions}

\begin{enumerate}
  \item Can we use a zero-shot approach to detect AI-generated text effectively?
  \item How does the detection performance vary across different models?
  \item How does the detection performance of a model compare when detecting text generated by itself versus text generated by other models?
\end{enumerate}

\paragraph{Possible extensions} We believe that the extensions will be discussed during the development of the project given that we have not yet been able to meet with our official TA. Currently, these are some of our ideas:
\begin{itemize}
  \item Change the model used to create the perturbations on the \textit{Fast-DetectGPT} algorithm;
  \item Use a dataset with a different language and use AI-generated text in that language.
\end{itemize}

\subsection*{Models and Datasets}

We will use three different models for both synthetic dataset generation and detection, specifically: DeepSeekR1 \cite{deepseekai}, GPT-OSS-20B \cite{gptoss20bmodel}, and Qwen3-4B 2507 \cite{Qwen3}.

When it comes to datasets, we will use HC3 \cite{HC3}, a large dataset containing questions with answers by both human and by GPT 3.5. The questions are separated by topics (finance, medicine, open question answering, Reddit and Wiki). We plan to choose a topic and sample a certain number of human sentences, and then use that to generate the synthetic datasets.

Apart from HC3, we also plan to use the four datasets originally tested by Mitchell et al.\cite{detectgpt}: WritingPrompts \cite{writingprompts} is a dataset with prompted stories and academic essays, SQuAD \cite{SQuAD} contains Wikipedia paragraphs, XSum \cite{XSum} contains news articles, and PubMedQA \cite{PubMedQA} is a group of long-form answers about biomedical research questions.

The metric used to measure performance on this task will be AUROC.

\subsection*{Workplan}

Our workplan will focus on: (1) Building synthetic datasets with the three different models; The generation of AI text can be done either by prompting a model to rephrase the human sentence or to continue generating words based on the first words of the sentence, (2) Implementation of the \textit{Fast-DetectGPT} algorithm and (3) Evaluation of the model's performance on the synthetic datasets with cross-evaluation, i.e., using each of the three models to detect AI-generated text from the other two models and also from itself.

\bibliographystyle{plain}
\bibliography{references}
% reference non cited references
\nocite{*}

\end{document}